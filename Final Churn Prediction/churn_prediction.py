# -*- coding: utf-8 -*-
"""Churn Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RWu-iQ5FiQiSGLe-3WWliP0MzLgNPMuN

#Setup
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

"""#Dataset"""

churn_dataset = pd.read_csv('https://raw.githubusercontent.com/sujin-lifology/customer-churn/main/telecom_customer_churn.csv')

churn_DatFrame = pd.DataFrame(churn_dataset)

"""# Information about the dataset

### No of Rows and Columns in the dataset
"""

churn_dataset.shape

"""### Having a look at the first view rows of the dataset"""

churn_dataset.head()

"""### Statistical Properties of our dataset"""

churn_dataset.describe()

"""# Data Pre-Processing

### Dropping Unwanted Row
"""

churn_dataset = churn_dataset[churn_dataset['Customer Status'] != 'Joined']

"""### Dropping Unwanted Columns"""

churn_dataset = churn_dataset.drop('Latitude', axis=1)
churn_dataset = churn_dataset.drop('Longitude', axis=1)
churn_dataset = churn_dataset.drop('City', axis=1)
churn_dataset = churn_dataset.drop('Offer', axis=1)
churn_dataset = churn_dataset.drop('Avg Monthly Long Distance Charges', axis=1)
churn_dataset = churn_dataset.drop('Online Security', axis=1)
churn_dataset = churn_dataset.drop('Online Backup', axis=1)
churn_dataset = churn_dataset.drop('Contract', axis=1)
churn_dataset = churn_dataset.drop('Internet Type', axis=1)
churn_dataset = churn_dataset.drop('Payment Method', axis=1)
churn_dataset = churn_dataset.drop('Total Refunds', axis=1)
churn_dataset = churn_dataset.drop('Total Extra Data Charges', axis=1)
churn_dataset = churn_dataset.drop('Churn Category', axis=1)
churn_dataset = churn_dataset.drop('Churn Reason', axis=1)
churn_dataset = churn_dataset.drop('Customer ID', axis=1)
churn_dataset = churn_dataset.drop('Gender', axis=1)

"""### Changing some values to Numerical Values"""

churn_dataset['Customer Status'] = churn_dataset['Customer Status'].replace({'Churned': 1, 'Stayed': 0})
churn_dataset['Married'] = churn_dataset['Married'].map({'Yes': 1, 'No': 0})
churn_dataset['Phone Service'] = churn_dataset['Phone Service'].map({'Yes': 1, 'No': 0})
churn_dataset['Multiple Lines'] = churn_dataset['Multiple Lines'].map({'Yes': 1, 'No': 0})
churn_dataset['Internet Service'] = churn_dataset['Internet Service'].map({'Yes': 1, 'No': 0})
churn_dataset['Device Protection Plan'] = churn_dataset['Device Protection Plan'].map({'Yes': 1, 'No': 0})
churn_dataset['Premium Tech Support'] = churn_dataset['Premium Tech Support'].map({'Yes': 1, 'No': 0})
churn_dataset['Streaming TV'] = churn_dataset['Streaming TV'].map({'Yes': 1, 'No': 0})
churn_dataset['Streaming Movies'] = churn_dataset['Streaming Movies'].map({'Yes': 1, 'No': 0})
churn_dataset['Streaming Music'] = churn_dataset['Streaming Music'].map({'Yes': 1, 'No': 0})
churn_dataset['Unlimited Data'] = churn_dataset['Unlimited Data'].map({'Yes': 1, 'No': 0})
churn_dataset['Paperless Billing'] = churn_dataset['Paperless Billing'].map({'Yes': 1, 'No': 0})

"""### Filling Missing values"""

columns_to_fill = ['Multiple Lines', 'Avg Monthly GB Download', 'Device Protection Plan', 'Premium Tech Support', 'Streaming TV', 'Streaming Movies', 'Streaming Music', 'Unlimited Data']

for col in columns_to_fill:
    if col == 'Avg Monthly GB Download':
        churn_dataset[col] = churn_dataset[col].fillna(churn_dataset[col].std())
    else:
        churn_dataset[col] = churn_dataset[col].fillna(churn_dataset[col].median())

churn_dataset

"""## Now Over Dataset looks good now We will train our dataset

# Training Dataset

### Splitting Input and output
"""

X_churn_data = churn_dataset.drop(columns = 'Customer Status', axis = 1)
Y_churn_data = churn_dataset['Customer Status']

"""### Splitting data for testing and training"""

X_training,X_testing,Y_training,Y_testing =  train_test_split(X_churn_data,Y_churn_data,test_size = 0.2, stratify=Y_churn_data, random_state=2)

"""### Applying Standardization"""

scaler = StandardScaler()

scaler.fit(X_training)

X_train_scaled = scaler.transform(X_training)
X_test_scaled = scaler.transform(X_testing)

"""### Training Model"""

classifier = svm.SVC(kernel='linear')

#training SVM Classifier
classifier.fit(X_training, Y_training)

# checking accuracy score for churn training data
X_churn_training_prediction = classifier.predict(X_training)
training_churn_data_accuracy = accuracy_score(X_churn_training_prediction, Y_training)

print('Accuracy score for traininig churn data : ', training_churn_data_accuracy)

# checking accuracy score for churn testing data
X_churn_testing_prediction = classifier.predict(X_testing)
testing_churn_data_accuracy = accuracy_score(X_churn_testing_prediction, Y_testing)

print('Accuracy score for testing churn data : ', testing_churn_data_accuracy)

"""### Evaluation Matrix"""

train_precision = precision_score(Y_training, X_churn_training_prediction)
train_recall = recall_score(Y_training, X_churn_training_prediction)
train_f1_score = f1_score(Y_training, X_churn_training_prediction)
train_conf_matrix = confusion_matrix(Y_training, X_churn_training_prediction)

# Calculate evaluation metrics for testing data
test_precision = precision_score(Y_testing, X_churn_testing_prediction)
test_recall = recall_score(Y_testing, X_churn_testing_prediction)
test_f1_score = f1_score(Y_testing, X_churn_testing_prediction)
test_conf_matrix = confusion_matrix(Y_testing, X_churn_testing_prediction)

"""### Evaluation Matrix Results"""

print("Evaluation metrics for training data:")
print("Precision:", train_precision)
print("Recall:", train_recall)
print("F1 Score:", train_f1_score)
print("Confusion Matrix:\n", train_conf_matrix)

print("\nEvaluation metrics for testing data:")
print("Precision:", test_precision)
print("Recall:", test_recall)
print("F1 Score:", test_f1_score)
print("Confusion Matrix:\n", test_conf_matrix)

